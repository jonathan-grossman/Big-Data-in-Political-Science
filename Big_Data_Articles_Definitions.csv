Authors,Article Name,Publication,Year,Definition Page(s),Definition,,
"Adolf, Marian T.; Stehr, Nico","Information, Knowledge, and the Return of Social Physics",ADMINISTRATION & SOCIETY,2018,1245,"What seems to have resuscitated the old idea of unmasking social reality in its totality by computing the sum of all its parts is the unprecedented availability of digital data on ever wider areas of social and natural life. Real-time monitoring through ubiquitous sensors and the outfitting of everyday appliances with information technology turns social spaces into places of ubiquitous surveillance. As more and more everyday activities are moved into the Internet, as web-based services are used to assist with our daily chores and professional activities, an ever bigger data trail is accumulated. Every telephone call, every online-order, every Google map-query is being logged. The use of Social Network Sites (SNS), of rebate and credit cards, to name just a few, produce activity-based “guest-ids” that hold a great deal of information. Just like meta-data of telephone calls and server log-ins might ultimately reveal more crucial information about an individual than the actual communications they result from, logging our interactions within contemporary media (or information) ecology tells a great deal about what people do. 

This phenomenon, presently discussed under the heading of Big Data, also in the public administration and governance literature (see, for example, Mergel, Rethemeyer, & Isett, 2016), is a token for the increasing informationalization of contemporary social life.",,
"Agostino, Deborah; Arnaboldi, Michela",Social media data used in the measurement of public services effectiveness: Empirical evidence from Twitter in higher education institutions,PUBLIC POLICY AND ADMINISTRATION,2017,297,"The key features of big data are high volumes of data, the high speed with which data are generated and the great variety of data formats, including, alongside traditional excel spreadsheets, data from sensors, mobile phones and social media (Gandomi and Haider, 2015; McAfee and Brynjolfsson, 2012).",,
"Blume, Grant; Scott, Tyler; Pirog, Maureen",Empirical Innovations in Policy Analysis,POLICY STUDIES JOURNAL,2014,S43,"The term “Big Data” represents multidimensional information, often on a massive scale, that is difficult to process or analyze using conventional empirical and statistical tools (Eaton, Deutsch, Deroos, Lapis, & Zikopoloulos, 2012).",,
"Campbell-Verduyn, Malcolm; Goguen, Marcel; Porter, Tony",Big Data and algorithmic governance: the case of financial practices,NEW POLITICAL ECONOMY,2017,220,"Varying definitions of Big Data mostly overlap in a stress on the ‘3Vs’ of volume, variety and velocity. Consisting of ‘ high-volume, high-velocity and high-variety information assets’ (Gartner 2013) beyond the processing capacity of human intelligence (Raley 2013: 132), Big Data yield correlations and patterns through statistical calculations of computer-coded algorithms (Kitchin 2014: 140). Algorithms are automated routines that mobilise information in patterned manners to achieve particular ends (Gillespie 2014). Both Big Data and algorithms involve expansions of information technologies that, while not entirely new, are widely seen as transformational, particularly in the degrees to which they enhance the monitoring of vast domains of human activity.",,
"Chandler, David",A World without Causation: Big Data and the Coming of Age of Posthumanism,MILLENNIUM-JOURNAL OF INTERNATIONAL STUDIES,2015,835-836,"While there is no fixed definition of Big Data, analysts often mention the 3 ‘Vs’ which characterise it: volume, velocity and variety. Big Data includes information from a multitude of sources, including social media, smart phones and mapping, visualising and recording equipment7 and the number of data-sharing devices is growing exponentially. This hardware, collectively known as the ‘Internet of Things’, includes machine sensors and consumer-oriented devices such as connected thermostats, light bulbs, refrigerators, and wearable health monitors.8 Data is thus being produced and used in increasingly diverse and innovative ways. The term ‘Big Data’ is capitalised to distinguish it (as a set of ideas and practices discursively cohered around a certain approach to knowledge production) from its use as a merely descriptive term for a large amount of data. Big Data thus is not used with reference to discussions about the volume of data per se; however, many authors argue that volume is relevant in terms of an analytical ‘tipping point’ or ‘data threshold’ where data gathering is no longer based upon selection and sampling with limited parameters but aspires to be exhaustive or becomes a closed data set, no longer requiring generative rules.9

Thus Big Data discursively refers to a qualitative shift in the meaning of data, in not just the amount of data (approaching exhaustiveness) but also its quality (approaching a dynamic, fine-grained relational richness). This data is very far from the abstract and reductionist constructions of data of the past:10 but is increasingly understood as approaching ‘reality’ itself. Thus, Big Data transforms our everyday reality and our immediate relation to the things around us. This ‘datafication’ of everyday life is at the heart of Big Data: a way of accessing reality through bringing interactions and relationships to the surface and making them visible, readable and thereby governable, rather than seeking to understand hidden laws of causality.11 Big Data is thereby generally understood to generate a different type of ‘knowledge’: more akin to the translation or interpretation of signs rather than that of understanding chains of causation.12",,
"Clark, William Roberts; Golder, Matt","Big Data, Causal Inference, and Formal Theory: Contradictory Trends in Political Science?",PS-POLITICAL SCIENCE & POLITICS,2015,66,"For us, “big data” refers to the idea that tech innovations such as machine learning have allowed scholars to gather either new types of data, such as social media data, or vast quantities of traditional data with less expense.",,
"Clarke, Amanda; Craft, Jonathan",The vestiges and vanguards of policy design in a digital context,CANADIAN PUBLIC ADMINISTRATION-ADMINISTRATION PUBLIQUE DU CANADA,2017,478,"big data—large-scale, unmediated and unstructured data",,
"Cukier, Kenneth; Mayer-Schoenberger, Viktor",The Rise of Big Data How It's Changing the Way We Think About the World,FOREIGN AFFAIRS,2013,29,"Given this massive scale, it is tempting to understand big data solely in terms of size. But that would be misleading. Big data is also characterized by the ability to render into data many aspects of the world that have never been quantified before; call it ""datafication."" For example, location has been datafied, first with the invention of longitude and latitude, and more recently with gps satellite systems. Words are treated as data when computers mine centuries' worth of books. Even friendships and ""likes"" are datafied, via Facebook.

This kind of data is being put to incredible new uses with the assistance of inexpensive computer memory, powerful processors, smart algorithms, clever software, and math that borrows from basic statistics. Instead of trying to ""teach"" a computer how to do things, such as drive a car or translate between languages, which artificial-intelligence experts have tried unsuccessfully to do for decades, the new approach is to feed enough data into a computer so that it can infer the probability that, say, a traffic light is green and not red or that, in a certain context, lumière is a more appropriate substitute for ""light"" than léger.

Using great volumes of information in this way requires three profound changes in how we approach data. The first is to collect and use a lot of data rather than settle for small amounts or samples, as statisticians have done for well over a century. The second is to shed our preference for highly curated and pristine data and instead accept messiness: in an increasing number of situations, a bit of inaccuracy can be tolerated, because the benefits of using vastly more data of variable quality outweigh the costs of using smaller amounts of very exact data. Third, in many instances, we will need to give up our quest to discover the cause of things, in return for accepting correlations. With big data, instead of trying to understand precisely why an engine breaks down or why a drug's side effect disappears, researchers can instead collect and analyze massive quantities of information about such events and everything that is associated with them, looking for patterns that might help predict future occurrences. Big data helps answer what, not why, and often that's good enough.",,
"Desouza, Kevin C.; Jacob, Benoy",Big Data in the Public Sector: Lessons for Practitioners and Scholars,ADMINISTRATION & SOCIETY,2017,1045-1047,"As a relatively new phenomenon, much of the literature on Big Data focuses on defining the “bounds” of Big Data. That is, what is Big Data and what does it mean to operate in a Big Data environment. Despite the ubiquity of the term, Big Data is a difficult term to define (Franks, 2012; Laney, 2001; Manyika et al., 2011). There is, however, some consensus among scholars and practitioners that four factors characterize Big Data—volume, velocity, variety, and complexity.5 More than just simple semantics, these characteristics have potentially important implications for management practices.

Big Data is just too big for us. Where . . . big data begin[s] and end[s] is not known. I have been struggling to identify digestible bites for us to take to move on big data . . . .

First, at its core, Big Data must clearly be “big.” Big Data datasets are “beyond the ability of typical database software tools to capture, store, manage and analyze” (Franks, 2012, p. 4).6 Thus, Big Data, in terms of volume, is a function of the underlying and pre-existing capacity of an organization to collect, store, and analyze its data. This definition suggests that Big Data is, in terms of volume, a moving target. For example, household demographics that were once difficult to manage now “fit on a thumb drive and can be analyzed by a low-end laptop” (Franks, 2012, p. 24).

The second defining characteristic of Big Data is its velocity. This refers to the speed at which data are being created and stored, and their associated rates of retrieval (Kaisler, Armour, Espinosa, & Money, 2013). Much like the volume of data, however, there is no established benchmark by which to consider when data velocity meets a Big Data threshold. Rather, the salient issue is that the data are being created at historically fast rates. An example of the current velocity of data is provided by Mayer-Schonberger and Cukier (2013):

Google processes more than 24 petabytes of data per day, a volume that is thousands of times the quantity of all printed material in the U.S. Library of Congress. Facebook, a company that didn’t exist a decade ago, gets more than 10 million new photos uploaded every hour. Facebook members click a “like” button or leave a comment nearly three billion times a day, creating a digital trail that the company can mine to learn about users’ preferences. Meanwhile, the 800 million monthly users of Google’s YouTube service upload over an hour of video every second. The number of messages on Twitter grows at around 200 percent a year and by 2012 exceeded 400 million tweets a day. (p. 8)

A third defining characteristic of Big Data is its variety. Big Data is comprised of data in a wide range of forms, including text, images, and videos. Generally speaking, then, it will include data that are structured, semi-structured, or unstructured. Structured data refer to data that have an organized structure and are, thus, clearly identifiable. A simple example would be a database with specific information that is stored in columns and rows. Semi-structured data do not conform to a formal structure per se; however, it contains “tags” that help separate the data records or fields. For example, data in many bibliographical software programs reflect semi-structured data. That is, the file is composed of records, but the structure is not regular in the sense that fields may be missing or be comprised of more “open” formats, such as a “Notes” section. Finally, unstructured data, as its name implies, have no identifiable structure. Examples of unstructured data, then, include the following: text messages, photos, videos, and audio files.

As datasets become increasingly “complex”—from structured to unstructured—the processing and analytical capabilities required to collect, manage, and analyze the data increases significantly. Thus, a better understanding of the defining characteristics of an organizations real, or potential, data repositories offers insights into the level and types of investments needed.

The final defining characteristic of Big Data is its complexity—the degree to which the data are interconnected. Many of the novel applications and/or insights that have emerged from Big Data applications are a result of connecting otherwise unrelated datasets. One oft-cited example is the joint effort between Google and the Center for Disease Control (CDC). In this case, Google was able to connect their database of “search terms”—entered in their search engine—such as “cold medicine” and “flu symptoms,” with the CDC’s data on the H1N1 virus. In doing so, analysts were able to predict the spread of the H1N1 virus by “connecting” two previously disconnected datasets.",,
"Durrant, Hannah; Barnett, Julie; Rempel, Emily Suzanne",Realising the Benefits of Integrated Data for Local Policymaking: Rhetoric versus Reality,POLITICS AND GOVERNANCE,2018,19,"Conventional attempts to define big data have tended to focus primarily on its characteristics; initially emphasising its volume, variety, and velocity (see Kitchin & McArdle, 2016). A more recent proliferation of characteristics identified with big data (e.g., Uprichard, 2013) has rendered the term more, rather than less, opaque (Kitchin & McArdle, 2016). In an attempt to isolate the most salient qualities of big data, Kitchin (2014b) stresses the distinction between small data sources—based on a population sample, infrequently collected and processed slowly and periodically—and big data that is both exhaustive (n = all) and generated and reported in close to real time. For Kitchin and McArdle (2016) the two most important characteristics of big data sources are exhaustivity and velocity.",,
"Eldridge, Christopher; Hobbs, Christopher; Moran, Matthew",Fusing algorithms and analysts: open-source intelligence in the age of 'Big Data',INTELLIGENCE AND NATIONAL SECURITY,2018,393-394,"In recent years, the term ‘Big Data’ has become a buzzword of sorts in discussions relating to developments online. Despite its ubiquity in academic and popular discourse, however, the term is poorly understood and often ill-defined. It has become a popular way to refer to the dramatic increase in recent years of the amount of data available online. But the concept represents more than vast quantities of information. McAfee and Brynjolfsson argue that the concept comprises three core factors: volume, variety and velocity. The issue of volume is perhaps the one most frequently highlighted in popular commentary due to the staggering figures involved. According to a prediction published in 2014 by an IT analysis company, the ‘digital universe’ doubles in size every two years and will reach 44 zettabytes by 2020.17 Indeed, for Kevjn Lim, it is the process of digitalisation that is the main driver behind the Big Data phenomenon. In 2000, 25 per cent of the world’s stored information was digital; by 2013, it was over 98 per cent. Equally important for understanding the nature of ‘Big Data’, however, is the variety of the data sources: ‘Big data takes the form of messages, updates and images posted to social networks; readings from sensors; GPS signals from cell phones, and more’. Furthermore, ‘many of the most important sources of Big Data are relatively new’. It is only since the mid-2000s that social networks such as Facebook and Twitter began to gain real momentum. Finally, the speed of data creation is regarded by many as even more important than the volume. On this issue of velocity, McAfee and Brynjolfsson note, ‘Real-time or nearly real-time information makes it possible for a company to be much more agile than its competitors’. This argument is stems from the business context but the logic applies across a range of fields; access to real-time information flows opens up unprecedented analytical opportunities.",,
"Gamage, Pandula",New development: Leveraging big data' analytics in the public sector,PUBLIC MONEY & MANAGEMENT,2016,385,"‘Big data’ is a general term referring to the massive amounts of data collected from many sources, including the web and the cloud (Manzoor, 2015). According to leading information technology and research company, Gartner (2015), big data is: 

…high-volume, high-velocity and/or high-variety information assets that demand cost-effective, innovative forms of information processing that enable enhanced insight, decision-making, and process automation. 

Gartner’s definition includes three characteristics known as the ‘three Vs’: the volume of information that systems must ingest process and disseminate; the velocity at which information grows or disappears; and the variety in the diversity of data sources and formats.",,
"Giest, Sarah",Big data for policymaking: fad or fasttrack?,POLICY SCIENCES,2017,367-368,"Big data is a broad term for the volume and complexity of data that is available. While there is no widely accepted definition of the term, the most basic description is that big data means datasets that are too large for traditional processing systems and require new technologies (Provost and Fawcett 2013). This not only refers to the size of the data, but also to its variety, velocity and veracity. This means that data is collected faster and that there is more variation of data that can be tapped into. Veracity refers to the uncertainty of data. This has to do both with the quality of the data, but also with the uncertainty of those dealing with the data of how accurate and complete this resource is.",,
"Gorham, Ashley E.",Big Data and Democracy: Facts and Values,PS-POLITICAL SCIENCE & POLITICS,2017,"961, n. 1","The most common definition of big data is “high volume, high velocity, and/or high variety information assets” (Gartner 2017). However, as Kitchin and McArdle (2016, 9) noted, “Big Data do no all share the same characteristics and...there are multiple forms of Big Data.” Eschewing technical terms, Clark and Golder (2015, 66) seem to express the general understanding of the term within political science with their conceptualization of big data as “the idea that technological innovations such as machine learning have allowed scholars to gather either new types of data, such as social media data, or vast quantities of traditional data with less expense.”",,
"Hansen, Hans Krause; Porter, Tony",What Do Big Data Do in Global Governance?,GLOBAL GOVERNANCE,2017,31,"Big data build on the exponential growth of data from new sources such as Internet clicks or machine sensors. They stand in contrast to conventional databases bounded and then managed according to specified standards and for predetermined purposes. Big data can involve ongoing streams of data that are processed, analyzed, and immediately supplied to users, in contrast to periodic surveys that may take months or years to process. Three distinctive features of Big data as compared to more conventional data have been identified: volume, variety, and velocity (3Vs).",,
"Hindman, Matthew","Building Better Models: Prediction, Replication, and Machine Learning in the Social Sciences",ANNALS OF THE AMERICAN ACADEMY OF POLITICAL AND SOCIAL SCIENCE,2015,48,"Many argue that the social sciences are on the brink of a “big data revolution.” A growing body of research now uses datasets that are “big” not just in terms of their size, but also in terms of their complexity, their variety, and the speed at which they accumulate.",,
"Hofferth, Sandra L.; Moran, Emilio F.; Entwisle, Barbara; Aber, J. Lawrence; Brady, Henry E.; Conley, Dalton; Cutter, Susan L.; Eckel, Catherine C.; Hamilton, Darrick; Hubacek, Klaus",Introduction: History and Motivation,ANNALS OF THE AMERICAN ACADEMY OF POLITICAL AND SOCIAL SCIENCE,2017,"6 (ABSTRACT), 8","Big data, that is, data that are byproducts of our lives rather than designed for research purposes...

...

big data—large, diverse, and heterogeneous datasets, often by-products generated from business and Internet transactions, email, social media, health care facilities, and various sensors and instruments",,
"Japec, Lilli; Kreuter, Frauke; Berg, Marcus; Biemer, Paul; Decker, Paul; Lampe, Cliff; Lane, Julia; O'Neil, Cathy; Usher, Abe",Big Data in Survey Research,PUBLIC OPINION QUARTERLY,2015,"840, 841-843","The term “Big Data” is an imprecise description of a rich and complicated set of characteristics, practices, techniques, ethical issues, and outcomes all associated with data.

...

In order to know when and how Big Data can be an appropriate technique for social insight, it is important to know more about the different features of Big Data. While there is no singularly preeminent Big Data definition, one very widely used definition comes from a 2001 Gartner report ( Laney 2001 , 2012 ) describing several characteristics of Big Data:

Volume refers to the sheer amount of data available for analysis. This volume of data is driven by the increasing number of data-collection instruments (e.g., social media tools, mobile applications, sensors) as well as the increased ability to store and transfer those data with recent improvements in data storage and networking.

Velocity refers to both the speed at which these data-collection events can occur, and the pressure of managing large streams of real-time data. Across the means of collecting social information, new information is being added to the database at rates ranging from as slow as every hour or so to as fast as thousands of events per second.

Variety refers to the complexity of formats in which Big Data can exist. Besides structured databases, there are large streams of unstructured documents, images, e-mail messages, video, links between devices, and other forms that create a heterogeneous set of data points. One effect of this complexity is that structuring and tying data together becomes a major effort, and therefore a central concern of Big Data analysis.

Others have added additional characteristics to the definition. These include Variability (inconsistency of the data across time), Veracity (ability to trust that the data are accurate), and Complexity (need to link multiple data sources).

There are many different types of Big Data sources; for example, social media data , personal data (e.g., data from tracking devices), sensor data , transactional data , and administrative data .

There are different opinions on whether administrative data should be considered to be Big Data or not. Administrative data are usually large in volume; they are generated for a different purpose and arise organically through administrative processes. Also, the content of administrative data is usually not designed by researchers. For these reasons, and because there is great potential in using administrative data, we will consider it to be in scope for this report.

There are a number of differences between administrative data and other types of Big Data that are worth pointing out. The amount of control a researcher has and the potential inferential power vary between different types of Big Data sources. For example, a researcher will likely not have any control of data from different social media platforms, and it could be difficult to decipher a text from social media. For administrative data, on the other hand, a statistical agency can form a partnership with owners of the data and influence the design of the data. Administrative data are more structured and well defined, and more is known about the data than perhaps other Big Data sources

A dimension of Big Data not often mentioned in the practitioner literature, but important for survey researchers to consider, is that Big Data are often secondary data, intended for another primary use. This means that Big Data are typically related to some non-research purpose and then reused by researchers to make a social observation. This is related to Sean Taylor’s distinction between “found vs. made” data ( Taylor 2013 ). He argues that a key difference between Big Data approaches and other social science approaches is that the data are not being initially “made” through the intervention of some researcher. When a survey researcher constructs an instrument, there are levels of planning and control that are necessarily absent in the data used in Big Data approaches. Big Data sources might have only a few variables, as compared with surveys that have a set of variables of interest to the researcher. In a 2011 Public Opinion Quarterly article and a blog post in his former role as director of the US Census Bureau, Robert Groves described a similar difference between organic and designed data ( Groves 2011a , 2011b ).",,
"Kernaghan, Kenneth","Digital dilemmas: Values, ethics and information technology",CANADIAN PUBLIC ADMINISTRATION-ADMINISTRATION PUBLIQUE DU CANADA,2014,300,"The Open Data movement is closely related to the Big Data (or Big Data analytics) movement dedicated to developing and exploiting data sets so large and complex that it is difficult to capture, store, manage and analyze them with typical database software.",,
"Kuiler, Erik W.",From Big Data to Knowledge: An Ontological Approach to Big Data Analytics,REVIEW OF POLICY RESEARCH,2014,311,"Following the Gartner Group’s definition, trade journals tend to emphasize three Big Data properties, collectively referenced as the three V’s :1 volume—to denote an exponentially large data set, ranging in size from one or more terabytes (1012) to multiple petabytes (1015) or exabytes (1018); velocity—to indicate data that arrive as continuous streams, rather than as transaction or database files; and variety—to designate data sets that contain both structured and unstructured data that may be subject to different semantics and in different formats, gathered from diverse sources. Discussing Big Data in its historical perspective, Jacobs (2009) offers a definition of the term that is perhaps more useful because it places Big Data in its proper IT context: “Big data should be defined at any point in time as ‘data whose size forces us to look beyond tried-and-true methods [of storage and manipulation] that are prevalent at that time.’”",,
"Lim, Kevjn",Big Data and Strategic Intelligence,INTELLIGENCE AND NATIONAL SECURITY,2016,621-622,"The term Big Data refers to massively voluminous, highly varied (i.e. structured and especially unstructured6 6 ‘Unstructured’ refers to data that are more complex to quantify such as photographs, video images, emails, text messages and so forth. Contemporary advances in automated analysis techniques, Bayesian-based machine learning and data mining allow for the datafication of such sources. According to one report, unstructured data make up over 90 per cent of the digital universe, see John Gantz and David Reinsel, ‘Extracting Value from Chaos’, IDC iView & EMC Corp. (June 2011) < http://www.emc.com/collateral/analyst-reports/idc-extracting-value-from-chaos-ar.pdf>. View all notes ) and dynamic real-time datasets that do not lend themselves to traditional relational data analysis processes. Instead, because of the orders of magnitude involved, the datasets are captured, ingested and interrogated across a number of servers and the results (of successive iterations) are re-aggregated afterwards in a procedure known as massive parallel processing (MPP) – the functional basis for Big Data analytics.

The key vector for the rise of Big Data is the digitization of information. In 2000, only a quarter of the world’s stored information was digital. In 2013, this figure rose to over 98 per cent of the approximately 1200 exabytes (1 exabyte equaling 1 billion gigabytes) of information stored 7 worldwide in all forms.7 But digitization is only a necessary, not sufficient condition for Big Data applications. What is still required is datafication that is the conversion of all structured, semi-structured and non-structured information packets into quantifiable units permitting the extraction of new forms of value.

",,
"Lin, Jimmy",On Building Better Mousetraps and Understanding the Human Condition: Reflections on Big Data in the Social Sciences,ANNALS OF THE AMERICAN ACADEMY OF POLITICAL AND SOCIAL SCIENCE,2015,35-36,"In the business context, big data are the (somewhat obvious) idea that an organization should retain data that result from carrying out its mission and exploit those data to generate insights for better decision-making. Also known as business intelligence, among other monikers, its origins date back several decades. In this sense, the big data hype is simply a rebranding of what many organizations have been doing for a long time. Today, these activities are known as data science and those who practice it are known as data scientists.

Examined more closely, however, there are three major trends that distinguish insight-generation activities today from, say, the 1990s. First, we have seen a tremendous explosion in the sheer amount of data—orders of magnitude increase. In the past, enterprises have typically focused on gathering data that are obviously valuable, such as business objects representing customers, items in catalogs, purchases, contracts, and so on. Today, in addition to such data, organizations also gather behavioral data from users. In the online setting, these include web pages that users visit and links that they click on, among others. The advent of social media and user-generated content, and the resulting interest in encouraging such interactions, further adds to the amount of data that is generated and collected. These are precisely the types of data that are valuable for computational social science.

Second, we see increasing sophistication in the types of analyses that organizations perform on their vast data stores. Traditionally, most information needs fell under what is known as online analytical processing (OLAP). Common tasks include creating joined views, followed by filtering, aggregation, or cube materialization— an example might be “show me the number of widgets sold in the northeast over the past six months to female customers.” We can characterize these activities as descriptive analytics, generating reports that an executive might consume. Today, data scientists are also interested in predictive analytics, which often involves building machine-learned models that can predict user behavior; for example, “what types of targeted ads can attract female customers to purchase this widget?” These models are then operationalized into data products that apply some sort of intervention (e.g., a recommender system) to hopefully affect user behavior. 

Finally, open-source software is playing an increasingly important role in today’s ecosystem. A decade ago, there was no credible open-source, distributed data analytics platform capable of handling large data volumes. Today, the open-source Hadoop platform, which began as an implementation of MapReduce (Dean and Ghemawat 2004), lies at the center of an ecosystem for large-scale data analytics, and is surrounded by complementary systems such as HBase, Pig, Hive, Spark, Giraph, and many others. Hadoop’s importance has been validated by its adoption in countless startups and mature enterprises. This broad base of support provides credibility, but the biggest impact of open-source infrastructure is the democratization of big data capabilities, especially when coupled with cloud computing. On-demand cloud services have obviated the need for many organizations to maintain dedicated hardware infrastructure, and today, analyses on terabytes of data can be conducted at modest costs without the need for major capital investments in servers. These tools are now within the reach of many social scientists, transforming the types of analyses they are able to conduct.",,
"Liu, Taoxiong; Xu, Xiaofei; Fan, Fangda",Forecasting Chinese GDP Using Online Data,EMERGING MARKETS FINANCE AND TRADE,2018,733,"Having a profound impact on social and economic development, the term big data refers to huge and diverse datasets (Ayi Armah 2013). The processing of these data extends beyond the abilities of current mainstream software, thereby requiring improvements in data processing tools.",,
"Longo, Justin; Dobell, Rod",The Limits of Policy Analytics: Early Examples and the Emerging Boundary of Possibilities,POLITICS AND GOVERNANCE,2018,6-7,"Definitions of ‘big data’ abound (Dutcher, 2014; Fredriksson, Mubarak, Tuohimaa, & Zhan, 2017; Ward & Barker, 2013), with most focusing on its characteristics— especially the large volume of data, its continuous flow at high velocity, and the variety of data available—and others pointing to the complexity of combined data sets and their value in revealing previously undetectable patterns. What emerges, however, from the policy analytics literature is a frequent conflation of ‘big data’ with ‘large’ data collections such as a census. While this reflects the current state of the art, our concept of big data draws especially on the velocity and variety (and, consequently, the large volume) of data as the foundation for a policy analytic approach that centres on a real-time understanding and interaction with the policy environment.",,
"Maciejewski, Mariusz","To do more, better, faster and more cheaply: using big data in public administration",INTERNATIONAL REVIEW OF ADMINISTRATIVE SCIENCES,2017,122-123,"There are several definitions of big data, ranging from the simple to the sophisticated. The simplest definition was formed by D. Laney, who described big data sets by their big volume, velocity and variety (Laney, 2001). This definition, while simple, is adequate for general purposes and is widely used (US Executive Office of the President, 2014). Fifteen years after this definition was proposed, datasets have become much bigger, more rapid and more varied. Almost everyone in modern society leaves digital traces of their lives, which may be gathered and may be available for easy processing and reasoning. The reasoning methods include aggregation of data, extraction, deduction, pattern detection, network analytics, trend evaluations, model creation, prediction and more. Methods of big data rely on applying certain reasoning to the above data sets with the help of computer technologies. In general, the core benefit of contemporary big data methods is that they allow for wide information input and intensive, automated analysis of that information. This allows an organization to manage and perform its duties significantly better, and even to execute tasks that would otherwise have been impossible. Successful application of big data methods results in:

1. A significant increase in the accuracy of decision-making, which occurs through:

a. The unprecedented expansion of the information database for analysing and drawing conclusions.

b. The feasibility of extensive work involving analysis and reasoning, which has been impossible to do with human resources alone.

c. The application of new methods of data presentation, allowing a better understanding of phenomena, changes over time and inter-relations.

d. The creation of algorithms to suggest appropriate solutions.

2. Significant acceleration of the performance of internal ‘information tasks’ through computerizing and automating data analysis and inference.

3. Significant reduction of the costs related to the decision-making process, thanks to the use of information technology rather than people to perform some or all of the analytical work and reasoning.

The opportunity to carry out tasks that were previously impossible is the consequence of the factors mentioned in points 1 and 2 above. To simplify: big data methods can uncover knowledge that was previously impossible to reveal. In turn, this new knowledge allows new tasks (previously impossible or even unimaginable) to be successfully carried out.",,
"Madsen, Anders Koed; Flyverbom, Mikkel; Hilbert, Martin; Ruppert, Evelyn",Big Data: Issues for an International Political Sociology of Data Practices,INTERNATIONAL POLITICAL SOCIOLOGY,2016,277,"Big data is a phenomenon that has been defined and conceptualized in various ways during the past five years ( De Mauro, Greco, and Grimaldi 2014 ). There is no consensus about what demarcates big data from other types of data, but a widely used definition has been Douglas Laney’s “3 v’s”:

Big data is high-volume, high-velocity, and/or high-variety information assets that require new forms of processing to enable enhanced decision-making, insight discovery, and process optimization ( Laney 2012 ).

According to Laney’s definition, big data practices are characterized by the technical attributes of the data they involve. Data points are numerous, and they come from various sources at a pace that is sometimes referred to as real time. This definition also entails that the problems raised by big data practices have technical solutions. For Laney, the pressing question is to find the right processing techniques to handle data with these new characteristics.

Writers such as Victor Meyer-Schönberger and Kenneth Cukier have challenged these largely technical definitions. Instead of focusing on the nature of data, they define big data with a focus on the new dynamics of knowledge production and valuation that are changed with the introduction of new data practices:

Big data refers to things one can do at a large scale that cannot be done at a smaller one, to extract new insights or create new forms of value, in ways that change markets, organizations, the relationship between citizens and governments, and more ( Mayer-Schönberger and Cukier 2013 ).

In line with our introductory comments above, we agree that the role of big data in transnational governance is not a purely technical phenomenon. It must be approached as a heterogeneous phenomenon that can be studied by attending to the interplay between social and technical relations in the way suggested by Meyer-Schönberger and Cukier. However, we want to add that this interplay can be fruitfully studied by attending to the following three moments of big data practices: 1) the datafication of daily life, 2) the production of patterns and predictions, and 3) the making of new modes of governance.",,
"Malomo, Fola; Sena, Vania",Data Intelligence for Local Government? Assessing the Benefits and Barriers to Use of Big Data in the Public Sector,POLICY AND INTERNET,2017,"7, 11, 12","It is now a popular byword for large volumes of data that are produced routinely by organizations and are too complex for standard software packages to process (Mayer- Schoenberger & Cukier, 2013).

...

The term Big Data emerged in the private sector around a decade ago, although there are earlier references to the concept (Laney, 2001). In reality, there is no agreed definition of Big Data. The most common one (Gartner, 2015) uses a series of Vs to describe the dimensions of Big Data:


Volume considers the amount of data generated and collected.
Velocity refers to the speed at which data are analyzed.
Variety indicates the different types of data that are collected.
Viscosity measures resistance to flow of data.
Variability measures the change of rate of flow
Veracity measures biases, noise, and abnormality.
Volatility indicates how long data are valid for and should be stored for.

However, there are additional features of Big Data that are essential to their nature and make them of interest. For instance, Manyika et al. (2011) use a subjective definition that emphasizes the size of the data sets being beyond the ability of typical database software tools to capture, store, manage, and analyze them. Other definitions focus on the fact that they can be of different types, and on the insights they can provide (Forbes, 2014).

...
As the variety of data produced by the public sector has grown, a few authors have pointed out that complexity—driven by the volume of data and the large number of data sources—is the defining feature of Big Data within the public sector (IBM, 2015) for several reasons (Sisense, 2013).",,
"Margetts, Helen",The Data Science of Politics,POLITICAL STUDIES REVIEW,2017,202-203,"Kitchin’s own definition of big data (Kitchin, 2014a, 2014b) is
more useful, laying out the distinguishing features of such data which allow new methodologies:

Huge in volume, consisting of terabytes or petabytes of data;

High in velocity, being created in or near real-time;

Diverse in variety, being structured and unstructured in nature;

Exhaustive in scope, striving to capture entire populations or systems (n?=?all);

Fine-grained in resolution and uniquely indexical in identification;

Relational in nature, containing common fields that enable the conjoining of different data sets;

Flexible, holding the traits of extensionality (can add new fields easily) and scaleability (can expand in size rapidly).",,
"McNeely, Connie L.; Hahm, Jong-on","The Big (Data) Bang: Policy, Prospects, and Challenges",REVIEW OF POLICY RESEARCH,2014,305,"In a pragmatic sense, big data has been used to describe datasets whose size is beyond the analytical capacities of most database software tools (Manyika et al., 2011), and the idea that things can be learned from a large body of data that cannot be comprehended from smaller amounts links big data to notions of complexity (cf. Kash & Rycroft, 2003). ",,
"Mergel, Ines; Rethemeyer, R. Karl; Isett, Kimberley",Big Data in Public Affairs,PUBLIC ADMINISTRATION REVIEW,2016,931,"Big data in public affairs, then, is high volume data that frequently combines highly structured administrative data actively collected by public sector organizations with continuously and automatically collected structured and unstructured real-time data that are often passively created by public and private entities through their Internet interactions.",,
"Miller, Peter V.",Is There a Future for Surveys?,PUBLIC OPINION QUARTERLY,2017,205 (ABSTRACT),...including administrative records and unstructured (“big”) data...,,
"Monroe, Burt L.; Pan, Jennifer; Roberts, Margaret E.; Sen, Maya; Sinclair, Betsy","No! Formal Theory, Causal Inference, and Big Data Are Not Contradictory Trends in Political Science",PS-POLITICAL SCIENCE & POLITICS,2015,71,"For us, the concept is broad and simultaneously captures several ideas. For us, the intuitive criterion—“lots of data”—is both unnecessary and insufficient. When referring to data qua data, “big” is defined relative to the computational or informational capacities of conventional approaches. Data can be big in many dimensions: observations (e.g., the US Census), covariates (e.g., gene sequences), file size (e.g., images), or network bandwidth (e.g., video) (Monroe 2013). Even “small” social data may contain interdependencies (e.g., networks, space-time, or hierarchy) that imply big models. The term also may describe the body of computational innovation that has become associated with such data, roughly equivalent to “data science” but need not imply the application of “data mining.” Among these innovations, we might list data collection (e.g., GPS tracking), data manipulation (e.g., web scraping), management (e.g., Hadoop), information extraction (e.g., natural language processing), or efficient computation (e.g., MapReduce), as well as the body of inferential techniques referred to variously as “statistical learning” or “machine learning.”",,
"Morris, Darcy Steeg",A Modeling Approach for Administrative Record Enumeration in the Decennial Census,PUBLIC OPINION QUARTERLY,2017,358,"The American Association for Public Opinion Research (AAPOR) considers administrative records to be big data because they are typically large in volume, “usually not designed by researchers,” and “generated for a different purpose and arise organically through administrative processes” (Japec et al. 2015).",,
"Patty, John W.; Penn, Elizabeth Maggie",Analyzing Big Data: Social Choice and Measurement,PS-POLITICAL SCIENCE & POLITICS,2015,95,"we begin by noting that empirical analysis of many interesting social science topics—and in particular those popularly associated with “big data” such as networks, text analysis, and genetics—always require data reduction. Data reduction is achieved through aggregating higher-dimensional data into lower-dimensional measures. In other words, we conceive of the “bigness” of data as a function of the data’s underlying conceptual structure. Much more than being a function of the number of observations or the number of variables, data is “big” if the concepts underlying the data—the data’s raison(s) d’être —are more complicated than a list of vectors of numbers. Social networks, texts, genomes, and brain scans all satisfy this requirement.",,
"Phillips, Fred",A perspective on 'Big Data',SCIENCE AND PUBLIC POLICY,2017,"730, 731","Today’s big data are popularly characterized by the first three Vs. Volumes and volumes of data. Instantaneous information on markets, metabolisms, and memes, and delivered to your PC or handheld in milliseconds, from naturally refrigerated server farms in Finland and under the Swiss Alps.

...

Bell (2014) rightly adds a fourth ‘v’ to the mix: validity. It is essential,
and difficult, to know whether the number in hand measures what it is supposed to measure.",,
"Pires, Sara Moreno; Magee, Liam; Holden, Meg",Learning from community indicators movements: Towards a citizen-powered urban data revolution,ENVIRONMENT AND PLANNING C-POLITICS AND SPACE,2017,1305,"The term “Big Data” is a simple sounding term that describes the massive accumulation of often apparently trivial information – public transport trips, social media tweets, local government payments and other mundane transactions that mark urban daily life. In aggregate, this data accumulation illustrates emergent patterns and trends, the digital “pulse” of the city. The volume, variety, veracity, and most of all, the velocity of this data revolution are taken to be both the effect and, increasingly, the cause of a city’s progress (IBM, 2015). This association of Big Data with rhetorics of success is supported by the acceleration of advanced levels of computing expertise that are required to manage, crunch and display these data flows. Technical experts exude varying levels of confidence that this pulse, if we develop the analytical tools to listen to it, can reveal secrets about cities, allowing greater predictability and ultimately controlling them from afar (Goldsmith and Crawford 2014; Kitchin, 2014; Sawyer, 2008).",,
"Poel, Martijn; Meyer, Eric T.; Schroeder, Ralph","Big Data for Policymaking: Great Expectations, but with Limited Progress?",POLICY AND INTERNET,2018,349,"The literature review, and the study as a whole, uses the following definition of Big Data: “Big Data is a step change in the scale and scope of the sources of materials (and tools for manipulating these sources) available in relation to a given object of interest” (Schroeder, 2014). This definition relates to a particular conception of “data,” whereby data belong to the objects investigated and they provide an “atomic” unit of analysis. This is a realist definition of data which highlights the scientific nature of data as evidence, with implications for debates about whether policy uses Big Data as evidence, or if not, for legitimating policies (which falls under the heading “policy?driven data” rather than the other way around, i.e., cherry?picking data to support a predetermined policy position, as suggested by our opening quote from Dickens, and which we discuss below), in which case its uses are not evidence based or scientific. In any event, this definition is more precise in the context of policymaking than the three or four “V's” which are often used in business and certain scientific contexts (Laney, 2001; Ward & Barker, 2013.), since not all efforts labeled as Big Data in the policy sphere entail high volume, velocity, variety and veracity, and also insofar as it focuses on the uses and applications of Big Data and focuses on the objects of interest to the researcher. Along the same lines, not all data sets are exhaustive in the sense of n?=?all.3",,
"Quintanilla, Gabriela; Gil-Garcia, Jose Ramon","Open Government and Linked Data: Concepts, Experiences and Lessons Based on the Mexican Case",REVISTA DEL CLAD REFORMA Y DEMOCRACIA,2016,75-76,"El término grandes datos se refiere a un gran conjunto complejo y cambiante de datos que crea nuevas formas de valor al cambiar los mercados, las organizaciones y las relaciones entre ciudadanos y Gobierno. Aprovechar los grandes datos implica la capacidad de analizar enormes cantidades de datos acerca de un tema (Mayer-Schönberger y Cukier, 2013).",,
"Rogge, Nicky; Agasisti, Tommaso; De Witte, Kristof",Big data and the measurement of public organizations' performance and efficiency: The state-of-the-art,PUBLIC POLICY AND ADMINISTRATION,2017,266-267,"In general, big data refers to huge volumes of (digital) data that are collected from large variety of sources that are too large, raw, or unstructured for analysis through conventional database techniques (Kim et al., 2014: 78). A common framework that is used to describe big data is the ‘3-V’ framework with the three dimensions ‘Volume’, ‘Variety’, and ‘Velocity’ (Brynjolfsson and McAfee, 2012; Chen et al., 2012; Gandomi and Haider, 2015; Kwon et al., 2014). In this framework, ‘Volume’ corresponds to the size of big data (typically multiple terabytes or petabytes). ‘Variety’, refers to the composition of the data set and, more in particular, to the structural heterogeneity in data (i.e. are the data structured, semi-structured, or unstructured). Practice shows that only a minority of the big data are structured. The ‘Velocity’ dimension refers to the dynamic nature of big data – the speed of collecting, storing and analyzing big data. Regarding this dimension, there is an increasing trend toward generating, collecting, storing, and analyzing data at high frequency (in some sectors and applications even real-time or near to real-time). While the volume or size dimension is most discussed in the context of big data, Gandomi and Haider (2015) stress that the other dimensions are equally important. In fact, they emphasize that one should avoid focusing exclusively on one particular dimension as there may be interactions between the dimensions. For instance, the interpretation of the ‘Volume’ dimension (i.e. when can a dataset be considered big data) may very well depend on whether the data are structured or not. Unstructured data usually require more storage and analysis capacities and better technologies than structured data. Therefore the threshold size for unstructured data will be smaller than for structured data. 

Next to the three dimensions of the basic 3-V framework, also other dimensions are sometimes used to characterize big data. Gandomi and Haider (2015) and Gani et al. (2015) describe four of these dimensions: ‘Veracity’ (unreliability and impreciseness of some data sources), ‘Variability’ (similar or dissimilar data flow rates), ‘Complexity’ (few or numerous data sources), and ‘Value’ (relative value density).",,
"Rumson, Alexander G.; Hallett, Stephen H.; Brewer, Timothy R.",Coastal risk adaptation: the potential role of accessible geospatial Big Data,MARINE POLICY,2017,"ABSTRACT, 102","‘Big Data’ involves powerful computer infrastructures, enabling storage, processing and real-time analysis of large volumes and varieties of data, in a fast and reliable manner. 

...

The terminology of Big Data, relates not only to the handling of large volumes of data, but refers also to broader data issues such as the ability to manage concurrently a wide variety of data formats, with the incorporation of high currency and even real-time (high velocity) data [43], [44]; allied to which are a new generation of analytical and data processing capabilities. Central to the concepts surrounding Big Data are the multi-node computer infrastructures employed, enabling the collective storage of large volumes of data in a distributed and scalable manner [45]. Big Data approaches can enable the large, diverse databanks associated with comprehensive environmental risk mapping exercises, to be incorporated successfully within a single modelling framework.",,
"Schintler, Laurie A.; Kulkarni, Rajendra","Big Data for Policy Analysis: The Good, The Bad, and The Ugly",REVIEW OF POLICY RESEARCH,2014,343,"Big data is ripe for policy analysis. While large, high-dimensional data sets have long been fundamental to research in the physical and life sciences, in fields like meteorology, biology, physics, chemistry, and astronomy, the use of big data beyond these disciplinary bounds has been much more limited. However, Web 2.0 services and technologies,1 as well as other recent cyber and technological developments, are producing an enormous and unprecedented amount of data that are rich in detail concerning human and societal behavior and related contextual factors and dynamics, including the attitudes, preferences, and sentiment of different individuals and groups (see, e.g., Bollen, Pepe, & Mao, 2009; Gondecha & Lieu, 2012). These data are not only voluminous, but also highly complex encompassing a variety of content formats (e.g., structured versus unstructured) and levels of spatial and temporal resolution. As a resource for helping to inform different points in the policy analysis process, from problem conceptualization to ongoing evaluation of existing policies, and even empowering and engaging citizens and stakeholders in the process, big data holds tremendous promise.",,
"Severo, Marta; Feredj, Amel; Romele, Alberto",Soft Data and Public Policy: Can Social Media Offer Alternatives to Official Statistics in Urban Policymaking?,POLICY AND INTERNET,2016,357,"Big Data is the most popular one, generally referring to data sets that are huge in volume, high in velocity, diverse in variety, and, although the introduction of this last aspect by M3 IBM3 is rather disputable, problematic in terms of veracity (4V definition). According to Kitchin (2014b), the emerging literature presents Big Data as: (i) exhaustive in scope; (ii) fine-grained in resolution; (iii) relational in NATURE; (iv) flexible and scalable. Hence, he argued in another article (Kitchin, 2014a, p. 2) that the term “Big” is somewhat misleading, since Big Data are characterized by much more than a huge volume. National census data sets, for instance, are very large in size, are exhaustive, and have a fine resolution, but they lack velocity, variety, and flexibility",,
"Stough, Roger; McBride, Dennis",Big Data and US Public Policy,REVIEW OF POLICY RESEARCH,2014,339,"A working definition adopted for this paper and one that has informally evolved in practical as well as scholarly circles is: Big Data may be viewed as databases that are too large to be adequately handled by current spread sheet technologies. As such Big Data sets are viewed by some to have limited use for public policy as well as other analytical purposes because of their irregular and heterogeneity properties (Schintler, 2013). As a consequence they tend to be inherently biased and lead to a conclusion that contemporary statistical analysis routines are inadequate to examine them. Furthermore, they cannot be adequately visualized, and it is often difficult to understand what the analyst is working with. In short, there is a signal to noise problem: Big Data sets are inherently noisy and thus their signal quality is poor.",,
"Swanson, Maureen H.; Vogel, Kathleen M.","Big Data, intelligence, and analyst privacy: investigating information dissemination at an NSA-funded research lab",INTELLIGENCE AND NATIONAL SECURITY,2018,"357-358, 369 n. 1.","LAS has an interdisciplinary research focus on the science of Big Data, a term that refers to ‘the collection of large and complex data sets and the analysis of these data sets for relationships’ [1]. Big Data can be analyzed computationally, either individually or integrated with other data-sets, to reveal hidden or previously unknown patterns, trends, and associations. Big Data can be collected through either actively (e.g., posts on social media or other types of user-initiated responses) or passively (e.g., FitBit data). Typically, there are four characteristics associated with Big Data, often described as the four Vs: Volume, Variety, Velocity, and Veracity. Volume refers to the ability of Big Data to consist of large, and increasingly larger, volumes of data (terabytes, petabytes, or zettabytes). Variety refers to the ability to make sense of different sources and types of data, including structured (e.g., numbers, dates, and groups of words and numbers) and unstructured (e.g., text, video, audio, tweets) forms that can come from a wide variety of classified and unclassified sources, to include public and privately held sources (sensors, computers and other electronic devices, social networks, the Web, mobile phones). This data can consist of information that is solely digital in origin, as well as data that have been subsequently digitized. Velocity refers to the increasing frequency of incoming data that needs to be analyzed, and the speed at which it can be analyzed (millisecond, second, minute, hour, day, week, month). Veracity signifies the trustworthiness of the data. LAS Big Data research in particular aims to advance and improve intelligence analysis capabilities on US national security threats.
...

[1] EPIC (Electronic Privacy Information Center), “Big Data”. The MacKensey Global Institute expands this definition to note that:

“Big Data” refers to datasets whose size is beyond the ability of typical database software tools to capture, store, manage, and analyze. This definition is intentionally subjective and incorporates a moving definition of how big a data-set needs to be in order to be considered Big Data – i.e., we don’t define Big Data in terms of being larger than a certain number of terabytes (thousands of gigabytes). We assume that, as technology advances over time, the size of datasets that qualify as Big Data will also increase. Also note that the definition can vary by sector, depending on what kinds of software tools are commonly available and what sizes of datasets are common in a particular industry. With those caveats, Big Data in many sectors today will range from a few dozen terabytes to multiple petabytes (thousands of terabytes).",,
"Titiunik, Rocio",Can Big Data Solve the Fundamental Problem of Causal Inference?,PS-POLITICAL SCIENCE & POLITICS,2015,75,"The term “big data” may be ascribed various meanings. In their contribution to this symposium, Patty and Penn understand big data as a highly multidimensional and complex body of information that is not inherently ordered and that necessarily must go through a process of data reduction before it can be analyzed. In contrast, in the de?nitions of big data that I consider, “big data” refers to a rectangular array of information with n rows and p columns. Thus, my notion of big data as a rectangle presupposes that the reduction discussed by Patty and Penn already has occurred and that appropriate decisions have been made to transform complex, non-ordered data into variables that take particular values for each observation—decisions that, as Patty and Penn point out in this issue, can be crucially consequential.

Another view of big data, and one that Ashworth, Berry, and Bueno de Mesquita adopt in their contribution to this symposium, refers to the statistical and computational tools of machine learning that typically are used to “mine” or extract structured patterns of information from large datasets.2 Although this is a valuable de?nition for many purposes, I do not engage it directly because my argument centers on whether the access to vast amounts of new information per se will solve the most fundamental obstacles to causal inference, and I consider this question to be largely independent of the available techniques to detect and characterize empirical regularities in big datasets.3

Instead, my focus is on two other interpretations of big data: big data “as large n” and big data “as large p.” The former refers to data sources with several observations—that is, records collected for hundreds of thousands or millions of units of analysis. In this interpretation, big data represents a situation in which the number of observations, n, is extremely large relative to the number of variables available in the dataset. In contrast, the latter interpretation refers to the availability of a large amount of information per observation—that is, data sources in which the number of variables, p, is very large relative to n.",,
"Van Puyvelde, Damien; Coulthart, Stephen; Hossain, M. Shahriar",Beyond the buzzword: big data and national security decision-making,INTERNATIONAL AFFAIRS,2017,"1399,  1400-1403","To take stock of the diverse perspectives on the subject, we rely on the efforts of De Mauro and her colleagues to define big data through a survey of 1,581 conference papers and journal articles on the topic. The resulting definition considers big data to be ‘the information assets characterized by such a high volume, velocity and variety to require specific technology and analytical methods for its transformation into value’.

...

The characteristics of big data: volume, velocity, variety and veracity

--------------------------------------------------
The expression ‘big data’ is often understood as a set of very large datasets. But what exactly qualifies as a very large dataset? Volume is understood and processed differently in multiple fields and at different points in time. For a social scientist, a dataset including hundreds of thousands of entries may seem large, but would not appear so to a computer scientist. Similarly, while computer scientists might have considered a database of hundreds of thousands of entries to be very large in the early days of computing, today's researchers work with billions of entries. A 2010 study found that the amount of data produced globally was 1.2 zettabytes, or 1,200,000,000,000 trillion gigabytes. It is expected that by the year 2020 worldwide data production will reach 35 zettabytes.16 The desire and ability to process such large volumes of data constitute a significant component of the definition of big data, but volume alone is not sufficient to define big data.

Early definitions of big data describe how large amounts of data put heavy demands on computing power and resources, thus causing a ‘big data problem’.17 As the world keeps producing more and more data, this problem is still with us. Processing capabilities for all these data now lag behind storage capabilities. In other words, we have access to massive amounts of data but are not able to use them all.18 Volume, therefore, should be considered not in isolation, but in relation to the ability to store and process data. The definition of big data must comprehend not only numbers or volume, but the capacity to use these data. Following this approach, a common definition of big data describes ‘datasets whose size is beyond the analytical capacities of most database software tools’.19 One national security professional explains that ‘big data’ starts when Excel is not enough any more.20

The capacity to use data is challenged not only by increasingly large volumes of data, but also by data velocity. The speed at which new data are generated and change is increasing, which poses further storing and processing challenges. Twitter users, for instance, generate on average 6,000 tweets per second.21 To cope with the velocity of data generation, researchers and intelligence practitioners have sought to combine and simultaneously process multiple data streams. In one such project, the NSA combined data from ‘phone conversations, military events, road-traffic patterns, public opinion—even the price of potatoes’.22 In this case, automated data analysis partially replaced humans, who could not process all these data in a timely fashion. According to a former official with knowledge of the program, analysts found that introducing more data into the program also led to more accurate predictions of where insurgent attacks would occur.

The ‘big data problem’ is further complicated by the growing variety of data. In computer sciences, data are generally considered as alphanumeric characters and symbols that are stored, processed and transmitted. In recent decades, the variety of data available to researchers has exploded, and technology has provided the means to tap into new data sources. The proliferation of smartphones and wearable technology connected to the internet and equipped with audio and video sensors as well as GPS locators is only one example of how technological advances produce more and more diverse data. Humans and their environment have always produced data, for example biological and meteorological data, but a growing number of sensors now collect and store these data in ever greater quantity and higher quality. In addition, the advent of cyberspace has led to new types of data, generated by networks and the humans who use them. Surfing on the World Wide Web generates digital trails of data that can be accessed by social media sites such as Twitter and search engines such as Google. The computer networks that constitute the digital layer of cyberspace also generate data, for example in the form of web server logs.23 Each of these data types, some older than others, poses specific challenges related to their volume and velocity, thus contributing to the increasing variety of data available for analysis.

Researchers usually classify data in three categories: structured, semi-structured and unstructured. Structured data, as described by one writer, have been reformatted and ‘organized into a data structure so that elements can be addressed, organized and accessed in various combinations to make better use of the information’.24 In other words, structured data have been processed so that they are easily stored for retrieval and analysis. Examples of structured data include texts and numeric information that are stored in traditional relational databases, meaning the data can fit in rows and columns. Quantitative social scientists often use structured data in the form of Excel spreadsheets. Structured data are the least common type of data but the most commonly analysed.

Semi-structured data are the next most common type; these are harder to store and analyse than structured data. This type of data is structured in so far as each data-object is represented by a certain number of attributes. For example, a Microsoft Word document with headed sections conveys semi-structured data. While this document lacks the rigid organization of structured data, it contains a number of attributes, including chapters or section headings, tags informing users about the date of creation, and revisions of the document, which can help users organize and analyse it.25

Unstructured data are the fastest-growing category and the most widespread. They come in a variety of formats including audio, video, analogue data, books, images, web pages and more. The proliferation of social media platforms and personal devices has increased the amount of publicly available unstructured data. This type of data poses a significant challenge to data scientists both inside and outside modern intelligence agencies. The former chief technology officer for the Central Intelligence Agency (CIA) sums up the problem for the US intelligence community:

Our data is always fragmented, and we're trying to make sense of fragmented data options, which is extremely difficult … how we analyze every piece of data, how we reprocess it to continue to make better sense of what is going on—that is the biggest [challenge] we have, especially when we can't get complete databases.26

Large organizations, such as Google and multiple government intelligence agencies, increasingly rely on novel types of databases that can store a wide variety of data types from structured to unstructured, and organize them for subsequent analysis.

Veracity is another focal point in the discussion on big data and national security. This characteristic refers to ‘the biases, noise and abnormality in data’.27 In the national security context, more than in any other field, data and information should be approached sceptically because adversaries often actively alter data with the intention to deceive and mislead. National security professionals must question the representativeness and validity of data collected on social media, for instance.28 Questioning the veracity of data encourages the consumers of big data to consider whether the data that are being stored and mined are reliable and meaningful enough to help them answer the problems at hand.29 A classic example of how data veracity can affect national security decisions comes from Robert McNamara's tenure at the Pentagon during the Vietnam War. McNamara oversaw a large-scale effort to collect data on the US war effort in Vietnam. While the term ‘big data’ was not in use at the time, his team of ‘Whiz Kids’ applied statistical techniques honed in business in an attempt to understand the war and assess the effectiveness of US decisions. Ultimately, however, McNamara put undue confidence in key metrics that he used to assess the success of US efforts in Vietnam, in particular body-count data on the enemy. According to one general, these data were susceptible to being distorted by commanders who wanted to demonstrate their effectiveness.30 McNamara himself notes in his memoirs that data used by the US military were ‘inflated by the considerable falsification of data submitted by South Vietnamese officials’.31 A more recent example of intentional data manipulation is provided by Gary King and his colleagues, who estimate that the Chinese government fabricates 448 million social media posts a year.32 Given the volume and diversity of big data, this is an inherently noisy environment, which reinforces the difficulty in identifying signals—in this case, genuine posts.33 The risk, if the veracity of big datasets cannot be established, is that they will mislead analysts, and possibly decision-makers.",,
"Washington, Anne L.",Government Information Policy in the Era of Big Data,REVIEW OF POLICY RESEARCH,2014,319,"Big data describes diverse unstructured digital collections interwoven to discover patterns. Big data may also refer to earlier knowledge-discovery tools such as data mining, machine learning, and predictive analytics.",,
"White, Patricia; Breckenridge, R. Saylor","Trade-Offs, Limitations, and Promises of Big Data in Social Science Research",REVIEW OF POLICY RESEARCH,2014,333,"In the current world of social science, the qualities of data that make it “big” are more distinct than just a large sample size or population level data. In addition to size, “big data” also has the qualities of being one or more of the following: relational, aggregated, multilevel, and merged (RAMM). Relational data have embedded linkages between individuals and groups; aggregated data include data and variables that are combined from multiple sources and measures; multilevel data combine measures at both individual and group levels; and merged data are combined from different original sources.",,
"Yeung, Karen",Algorithmic regulation: A critical interrogation,REGULATION & GOVERNANCE,2018,505,"Although ""Big Data"" has been variously defined, I use the term to refer to the sociotechnical ensemble that utilizes a methodological technique that combines a technology (constituted by a configuration of information-processing hardware and software that can sift and sort vast quantities of data in very short times) with a process (through which algorithmic processes are applied to mine a large volume of digital data to find patterns and correlations within that data, distilling the patterns into predictive analytics, and applying the analytics to new data) (Cohen 2012, p. 1919). The excitement surrounding Big Data is rooted in its capacity to identify patterns and correlations that cannot be detected by human cognition, converting massive volumes of data ( often in unstructured form) into a particular, highly data-intensive form of knowledge, and thus creating a new mode of knowledge production (Cohen 2012, p. 1919).",,
"Youtie, Jan; Porter, Alan L.; Huang, Ying",Early social science research about Big Data,SCIENCE AND PUBLIC POLICY,2017,65-66,"Big Data is defined on the basis of size, growth, diversity, and analytic capacities.

...

Although the legacy of information technology developments is long, the term ‘Big Data’ has a more recent history that some trace to a special issue of Nature published in September 2008, on the topic, while others allude to earlier or later references. Indeed the term itself has become a ‘meme’ for developments in the 21st century that facilitate the procurement, storage, processing, and analysis of large-scale information compilations. Boyd and Crawford (2012) call out the ‘mythology’ of the term, associating it with an overly optimistic and opportunistic rhetoric. The White House (2014) has drawn on the Gartner Inc. definition of Big Data in terms of the three ‘Vs’ (although more V’s have been added in other definitions):

volume of data collected and processed at a decreasing cost

variety of data, including digital data and data originating in analog forms that can be digitized (see President’s Council of Advisors on Science and Technology 2014)

velocity of data that can be obtained nearly in real-time

The ability to process more information, more quickly, and with greater ease of analysis opens up opportunities in areas such as: medical, business, scientific research, environmental, defense, and climate change applications (Bryant et al. 2008).
Yet it is not solely a matter of scale and opportunity that defines Big Data. It also matters how these methods are used, the extent to which they are divorced from context, applications that could violate privacy and security expectations, and inequities and unethical consequences that these capabilities could create. In sum, important societal considerations accompany Big Data development and applications. Such considerations are not solely the purview of Big Data. Observers underscore the importance of societal considerations in emerging technologies ranging from human genomics to nanotechnology to geoengineering.",,
"Zeng, Jinghan",China's date with big data: will it strengthen or threaten authoritarian rule?,INTERNATIONAL AFFAIRS,2016,1444-1445,"While big data may also include traditional (non-digital) sources of data, this article will primarily focus on digital ones. Thus, big data broadly refers here to an ‘explosion in the quantity and diversity of high frequency digital data’.",,
"Zwitter, Andrej",Big Data and International Relations,ETHICS & INTERNATIONAL AFFAIRS,2015,378-380,"What Is Big Data?
Big Data refers to the enormous amounts of data that, using sophisticated analytics techniques, can be mined for information in order to reveal patterns and spot trends and correlations. A key idea behind the concept is that the sheer volume of data allows users to discover information—specifically, correlations and patterns—that would not be available by looking at smaller samples. It also relates to the enhanced ability to extract information from, and interpret, massive amounts of unstructured data. Another key idea is that Big Data is updated in near real-time. The most important features relevant to understanding Big Data are known as “the three Vs,” which are: 3

Volume: Data today exists in massive amounts, which can be measured in petabytes [1015 bytes], exabytes [1018 bytes], and zetabytes [1021 bytes]. Soon, it will even be measured in yottabytes [1024], one of which equals 250 trillion digital video disks. According to Rick Smolan and Jennifer Erwitt, from the beginning of history until 2011, five billion gigabytes were produced; in 2015 this amount is produced every ten seconds. 4

Velocity: The speed of data creation and its collection now approaches real-time. This concerns not only questions of bandwidth (megabyte and gigabyte upload and download capabilities) but also of implementing information-technology architecture solutions that can cope with data in near real-time.

Variety: Data exists in structured and unstructured forms and in different formats and units of analysis, including documents, emails, social media messages, YouTube videos, pictures, audio, radio-frequency identification chips, 5 satellite imagery, sky cartography, DNA sequencing, phone-network call data, and cell phone GPS signals. Furthermore, it can be categorized depending on its source: for example, there is self-generated data, data collected (mostly in automatized ways) from the web (known as data scraping), and data retrieved from other outside sources.

Some authors add a number of other features that are potentially, but not necessarily, present in Big Data:

Veracity: This fourth V encompasses all sorts of methodological questions about the reliability and validity of data and its sources. For example, possible biases can be generated by the form of collection (do sentiment analyses generated from a certain region based on Twitter feeds actually express the sentiments of that region, or only those of Twitter users from that region?), and the form of data preparation (for example, by removing duplicates, completing partial entries, aggregating results, and so on).

Value: Oracle, for example, considers Big Data to be of low value density; that is, the data received and collected requires much processing before value can be extracted from it. 6

Correlations: Algorithms that analyze Big Data emphasize correlations over causation for the use of predictions and for social engineering, that is, the manipulation of individuals and groups based on social mechanisms. 7

Exhaustiveness: Related to Volume, Big Data becomes potentially all-encompassing of a research population, unlike with statistics, which commonly works with samples that only represent and approximate the whole—that is, the sample size is moving toward a state of N = all. 8

Detailed/Organic Data: Unlike Exhaustiveness, which is about sample size, organic data refers to the degree of granularity of the data within the sample; increasingly, organic data allows for a more accurate digital representation of physical reality in such fine detail that it approaches an organic representation of reality—metaphorically, a 1:1 map of the world. 9

Flexibility: Big Data encompasses extensionality (new dimensions can be added by adding new datasets) and scalability (the size of datasets can be expanded rapidly). 10

Virality: This factor measures how quickly data is spread and shared across networks of people (P2P) to each unique node. The time and rate of the spread of information are the determinant factors. 11 Viral diffusion differs from broadcasting in that the former, acting through a network structure, allows its individual nodes to contribute to social cascades, resulting in viral stories and posts. 12

In sum, Big Data promises an information advantage, be it in business intelligence, state intelligence, or any other form of data gathering and analysis. At least in theory, it offers the ability to become omniscient, if not omnipotent—a promise that is tempting to any business, government, or criminal network.",,
